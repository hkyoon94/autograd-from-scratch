from enum import Enum
from typing import overload, Optional, Sequence

import numpy as np


class Device(Enum):
    CPU = ...
    CUDA = ...


# Tensor
class Tensor:
    @staticmethod
    @overload
    def __init__(buf: memoryview, shape: Sequence[int]) -> Tensor: ...  # overload
    @staticmethod
    @overload
    def __init__(shape: Sequence[int]) -> Tensor: ...  # overload
    @staticmethod
    @overload
    def __init__(shape: Sequence[int], value: float) -> Tensor: ...  # overload
    def to(self, device: Device) -> Tensor: ...
    def broadcast_to(self, new_shape: Sequence[int]) -> Tensor: ...
    def contiguous(self) -> Tensor: ...
    def backward(self, init_grad: Optional[Tensor] = None) -> None: ...
    def getelement(self, args: Sequence[int]) -> float: ...
    def from_numpy(self, arr: np.ndarray) -> Tensor: ...
    def numpy(self) -> np.ndarray: ...
    def check_nan(self) -> None: ...
    def __repr__(self) -> str: ...
    @property
    def shape(self) -> Tensor: ...  # Read-only
    @property
    def stride(self) -> Tensor: ...  # Read-only
    @property
    def numel(self) -> int: ...  # Read-only
    @property
    def is_contiguous(self) -> bool: ...  # Read-only
    @property
    def grad(self) -> Tensor: ...  # Read-write
    @property
    def grad_fn(self) -> Function: ...  # Read-write
    @property
    def requires_grad(self) -> bool: ...  # Read-write


# Functions
class Function:
    # TODO: Enhance interface
    def forward(self, args: list[Tensor]) -> Tensor: ...
class View(Function):
    def forward(self, args: list[Tensor], as_shape: list[int]) -> Tensor: ...
class Broadcast(Function):
    def forward(self, args: list[Tensor], as_shape: list[int]) -> Tensor: ...
class Div(Function):
    def forward(self, args: list[Tensor], divisor: list[int]) -> Tensor: ...
class Sum(Function):
    def forward(self, args: list[Tensor], dims: list[int]) -> Tensor: ...
class Add(Function):
    def forward(self, args: list[Tensor]) -> Tensor: ...
class MatMul(Function):
    def forward(self, args: list[Tensor]) -> Tensor: ...
class Sigmoid(Function):
    def forward(self, args: list[Tensor]) -> Tensor: ...
class LeakyRelu(Function):
    def forward(self, args: list[Tensor]) -> Tensor: ...
class SoftmaxCrossEntropyMean(Function):
    def forward(self, args: list[Tensor]) -> Tensor: ...


# Optimizers
class SGDOptimizer:
    def __init__(self, params: list[Tensor], lr: float): ...
    def step() -> None: ...
    def zero_grad() -> None: ...
    @property
    def params(self) -> list[Tensor]: ...
    @property
    def lr(self) -> float: ...


# Computational Graph
class ComputationalGraph:
    @property
    def edges(self) -> list[tuple[str, str, list[int]]]: ...


# Autograd
class AutogradEngine:
    @staticmethod
    def on(flag: bool) -> None: ...
    @staticmethod
    def track_graph(flag: bool) -> None: ...
    @staticmethod
    def get_graph() -> ComputationalGraph: ...


# Memory
def _flush_temp() -> None: ...
def _flush_persistent() -> None: ...

 
# Basic ops
def zeros(shape: Sequence[int]) -> Tensor: ...
def zeros_like(t: Tensor) -> Tensor: ...
def ones(shape: Sequence[int]) -> Tensor: ...
def ones_like(t: Tensor) -> Tensor: ...
def randn(t: Tensor, std: float) -> Tensor: ...

# Operators
# sum
def sum(x: Tensor, dims: Sequence[int]) -> Tensor: ...
def sum_backward(grad: Tensor, x: Tensor) -> Tensor: ...

# add
def add(a: Tensor, b: Tensor) -> Tensor:...
def add_backward(grad: Tensor, a: Tensor, b: Tensor) -> Sequence[Tensor]:...
    # -> broadcasting enabled
def add_inplace(a: Tensor, b: Tensor) -> Tensor:...

# mm
def mm(a: Tensor, b: Tensor) -> Tensor: ...
def mm_backward(grad: Tensor, a: Tensor, b: Tensor) -> Sequence[Tensor]: ...

# bmm #TODO: enable bached matmul combining with view ops

# Activations
def sigmoid(a: Tensor) -> Tensor: ...
def sigmoid_backward(grad: Tensor) -> Tensor: ...
def leaky_relu(a: Tensor) -> Tensor: ...
def leaky_relu_backward(grad: Tensor) -> Tensor: ...

# Nonlinear
def ce_softmax_mean(logits: Tensor, labels: Tensor) -> tuple[Tensor, ...]: ...
def ce_softmax_mean_backward(grad: Tensor, logits: Tensor, labels: Tensor) -> Tensor: ...
